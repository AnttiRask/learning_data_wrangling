---
title: "2 Loading and Exploring Datasets"
author: "Gustavo R. Santos (original) | Antti Rask (modifications)"
date: "2022-06-20"
output: html_document
---

# Loading Files

## Load Libraries

```{r, message=FALSE, warning=FALSE}
library(conflicted) # An Alternative Conflict Resolution Strategy
  conflicts_prefer(dplyr::filter)
library(data.table) # Extension of `data.frame`
library(GGally)     # Extension to 'ggplot2'
library(httr)       # Tools for Working with URLs and HTTP
library(jsonlite)   # A Simple and Robust JSON Parser and Generator for R
library(lindia)     # Automated Linear Regression Diagnostic
library(magrittr)   # A Forward-Pipe Operator for R
library(mice)       # Multivariate Imputation by Chained Equations
library(rvest)      # Easily Harvest (Scrape) Web Pages
library(tidyverse)  # Easily Install and Load the 'Tidyverse'
```

## Load a built-in dataset

```{r}
df <- Orange
```

## Save the dataset to a CSV

```{r}
write_csv(df, "data/oranges_saved.csv")
```

## Loagind CSV files

### Load dataset from CSV

```{r}
path    <- "data/oranges.csv"
df_base <- read.csv(path)
df_tbl  <- read_csv(path)
df_dt   <- fread(path)
```

### Load dataset with missing values and specific rows/columns

```{r}
path2 <- "data/oranges_modified.csv"

df_na <- read_csv(path2, na = c("", "No Record")) %>%
  mutate(circumference...4 = as.numeric(circumference...4))

df_na
```


```{r}
df_restricted <- read_csv(
  path2,
  col_select = c(1, 2, 3),
  n_max      = 10
)

df_restricted
```

## Tibble vs Dataframe

### Printing

```{r}
data("world_bank_pop")
world_bank_pop
```

```{r}
View(world_bank_pop)
```

### Column Names

```{r}
data.frame(`my column` = c(1, 2, 3))
```

```{r}
tibble(`my column` = c(1, 2, 3))
```

## Comparisons: read.csv() or read_csv()?

### Create a large dataset

```{r}
n_observations <- 735999

data_test <- data.frame(
  ID   = 1:n_observations,
  var1 = runif(n_observations),
  var2 = rexp(n_observations),
  var3 = rnorm(n_observations)
)

data_test
```

```{r}
write_csv(data_test, "data/test_data.csv")
```

### Compare loading times

```{r}
time_read.csv <- system.time(df <- read.csv("data/test_data.csv"))[3]
time_read_csv <- system.time(df <- read_csv("data/test_data.csv"))[3]
time_fread    <- system.time(df <- fread("data/test_data.csv"))[3]

comparison <- tibble(
  name = factor(
    c("read.csv", "read_csv", "fread"),
    levels = c("read.csv", "read_csv", "fread")
  ),
  time = c(time_read.csv, time_read_csv, time_fread)
)

comparison %>%
  ggplot(aes(name, time)) +
  geom_col(fill = "royalblue") +
  scale_y_continuous(
    expand = c(0, 0),
    breaks = seq(0, 16, by = 2)
    ) +
  labs(
    title = "Comparison of time elapsed to read CSV file",
    x     = NULL,
    y     = "Time in Seconds"
  ) +
  theme_classic() +
  theme(
    axis.line.x  = element_blank(),
    axis.ticks.x = element_blank()
  )
```

# A Workflow for Data Exploration

## Texas Housing Sales

```{r}
# Import Texas Housing Sales
data("txhousing")

# View the dataset
txhousing
```

```{r}
# Check data types
glimpse(txhousing)
str_glue("\n")
str(txhousing)
```

```{r}
# Adjusting data types
txhouses <- txhousing %>%
  mutate(
    city = as.factor(city),
    date = date_decimal(date, tz = "GMT") %>% as_date()
  )

glimpse(txhouses)
```

```{r}
# Descriptive statistics
summary(txhouses)
```

```{r}
# Does each city appear the same number of times? 187
nrow(txhouses) / length(unique(txhouses %>% pull(city)))
```

## Missing Values

```{r}
idx_notna <- txhouses %>% 
  mutate(idx = row_number()) %>%
  na.omit() %>%
  pull(idx)

# View all the NAs
txhouses %>% 
  filter(!row_number() %in% idx_notna)
```

```{r}
# Percentage of NAs
txhouses %>%
  filter(!row_number() %in% idx_notna) %>%
  nrow() / nrow(txhouses)
```

```{r}
# Filter out the rows with 5 NAs
txhouses_filtered <- txhouses %>%
  filter(rowSums(is.na(.)) != 5)

txhouses_filtered
```

```{r}
# Proportions of NAs
proportions_na <- txhouses_filtered %>%
  select(listings, inventory, sales, volume, median) %>%
  summarise(across(everything(), ~ mean(is.na(.)))) %>%
  pivot_longer(
    everything(),
    names_to  = "column",
    values_to = "proportion_na"
  )

proportions_na %>%
  pwalk(function(column, proportion_na) {
    print(
      str_glue(
        "\nProportions to {column}:      \n",
        "\nFALSE {1 - proportion_na}     \n",
        "TRUE  {proportion_na}           \n",
        "--------------------------------\n"
      )
    )
  })
```

## Imputations

```{r}
# Impute median value
txhouses_imputed <- txhouses_filtered %>% 
  mutate(
    median = if_else(
      is.na(median), median(median, na.rm = TRUE), median
    ),
    sales  = if_else(
      is.na(sales), median(sales, na.rm = TRUE), sales
    ),
    volume = if_else(
      is.na(volume), median(volume, na.rm = TRUE), volume
    )
  )
```

```{r}
# Use {mice} imputation
impute      <- mice(data = txhouses_imputed %>% select(7:8), seed = 123)
impute_data <- complete(impute, 1) %>% as_tibble()
```

```{r}
# Replace the columns with the imputed ones
txhouses_clean <- txhouses_imputed %>%
  select(-c(listings, inventory)) %>%
  bind_cols(impute_data) %>%
  select(city:median, listings, inventory, date)

txhouses_clean
```

```{r}
# Let's compare the distribution of the data prior and after the imputation
comparison_after_impute <- tibble(
  before = txhouses_filtered %>% pull(listings),
  after  = txhouses_clean    %>% pull(listings)
)

before <- density(na.omit(comparison_after_impute %>% pull(before)))
after  <- density(na.omit(comparison_after_impute %>% pull(after)))

# Create a tibble for plotting
plot_data <- tibble(
  x = c(before$x, after$x),
  y = c(before$y, after$y),
  group = factor(rep(c("Before", "After"), each = length(before$x)))
)

# Plot both distributions using ggplot2
plot_data %>% 
  ggplot(aes(x, y, color = group)) +
  geom_line() +
  scale_color_manual(values = c("Before" = "red", "After" = "blue")) +
  labs(
    title = "Comparison Before and After Imputation Listings and Inventory",
    x     = NULL,
    y     = "Density"
  ) +
  theme_classic() +
  theme(legend.title = element_blank())
```

```{r}
# Checking missing data
sum(is.na(txhouses_clean))
```

## Data Distributions

### Histograms

```{r}
txhouses_clean %>% 
  select(-c(city, date)) %>% 
  pivot_longer(
    cols      = everything(),
    names_to  = "key",
    values_to = "value"
  ) %>%
  mutate(key = str_to_title(key)) %>%
  ggplot(aes(value)) +
  geom_histogram(fill = "blue", color = "white") +
  facet_wrap(vars(key), scales = "free") +
  labs(
    x = NULL,
    y = NULL
  ) +
  theme_classic()
```

### Boxplots

```{r}
txhouses_clean %>%
  select(listings, inventory, sales, volume, median) %>%
  pivot_longer(
    cols      = everything(),
    names_to  = "key",
    values_to = "value"
  ) %>%
  mutate(key = str_to_title(key)) %>% 
  ggplot(aes(key, value)) +
  geom_boxplot(fill = "blue", color = "white") +
  facet_wrap(vars(key), scales = "free") +
  labs(
    x = NULL,
    y = NULL
  ) +
  theme_classic()
```

## Checking Quantity and Removing Outliers

### Define a function to calculate and print the outlier count

```{r}
print_outlier_count <- function(tbl, var) {
  
  outliers <- tbl %>%
    summarize(
      outliers = sum(.data[[var]] < quantile(.data[[var]], 0.25) - 1.5 * IQR(.data[[var]])) +
        sum(.data[[var]] > quantile(.data[[var]], 0.75) + 1.5 * IQR(.data[[var]]))
    )
  
  print(
    str_glue(
      "Variable: {var} \n",
      "Outlier observations: {outliers} | Pct: ",
      "{round(outliers / nrow(tbl), 3) * 100} % \n",
      "----------------------------------------"
    )
  )
}
```

### Define the create_outlier_condition function

```{r}
create_outlier_condition <- function(tbl, var) {
  
  lower_cap <- quantile(tbl[[var]], 0.25, na.rm = TRUE) - 1.5 * IQR(tbl[[var]], na.rm = TRUE)
  upper_cap <- quantile(tbl[[var]], 0.75, na.rm = TRUE) + 1.5 * IQR(tbl[[var]], na.rm = TRUE)
  
  print_outlier_count(tbl, var)
  
  tbl %>%
    mutate(is_outlier = .data[[var]] <= upper_cap & .data[[var]] >= lower_cap) %>%
    pull(is_outlier)
}
```

### Specify the variables to process

```{r}
vars_to_process <- c("inventory", "listings", "median", "sales", "volume")
```

### Create filtering conditions for each variable

```{r}
outlier_conditions <- map(
  vars_to_process,
  ~create_outlier_condition(txhouses_clean, .x)
)
```

### Combine the filtering conditions using the reduce() function

```{r}
combined_conditions <- reduce(outlier_conditions, `&`)
```

### Apply the combined filtering condition to the original data frame

```{r}
txhouses_no_outliers <- txhouses_clean %>%
  filter(combined_conditions)

txhouses_no_outliers
```

## Visualizations

### Correlations

```{r}
correlation_matrix <- txhouses_no_outliers %>%
  select(-c(city, date)) %>% 
  cor() %>%
  as.data.frame() %>%
  rownames_to_column(var = "Var1") %>%
  pivot_longer(
    cols      = -Var1,
    names_to  = "Var2",
    values_to = "Correlation"
  ) %>% as_tibble()

correlation_matrix %>%
  ggplot(aes(Var1, Var2, fill = Correlation)) +
  geom_tile(color = "black") +
  geom_text(aes(label = round(Correlation, 2)), color = "black") +
  scale_fill_gradient2(
    low      = "red",
    high     = "blue",
    mid      = "white",
    midpoint = 0,
    limits   = c(-1, 1)
  ) +
  labs(
    fill = "Correlation",
    x    = NULL,
    y    = NULL
  ) +
  theme_classic()
```

### Scatterplots

```{r}
txhouses_no_outliers %>%
  select(median, sales, volume, listings, inventory) %>%
  ggpairs(progress = FALSE) +
  theme_bw()
```

## Modeling

### Train Test Split

```{r}
set.seed(123)
idx   <- sample(1:nrow(txhouses_no_outliers), nrow(txhouses_no_outliers) * 0.8)
train <- txhouses_no_outliers[idx, ]
test  <- txhouses_no_outliers[-idx, ]
```

### Linear Regression

```{r}
linear_model <- lm(median ~ city + year + month + volume + inventory, data = train)

summary(linear_model)
```

```{r, message=FALSE, warning=FALSE}
# Plot the residuals
linear_model %>%
  gg_diagnose(theme = ggplot2::theme_classic())
```

### Model Evaluation (extra)

```{r}
# Predict on test data
test_predictions <- predict(linear_model, newdata = test)

# Calculate RMSE
rmse <- sqrt(mean((test %>% pull(median) - test_predictions) ^ 2))
rmse
```

```{r}
# Create a data frame for residuals
residuals_tbl <- tibble(
  Predicted = test_predictions,
  Residuals = test %>% pull(median) - test_predictions
)

# Plot the residuals
residuals_tbl %>%
  ggplot(aes(Predicted, Residuals)) +
  geom_point() +
  geom_hline(
    yintercept = 0,
    color      = "red", 
    linetype   = "dashed"
  ) +
  labs(
    title = "Residuals",
    x     = "Predicted Values",
    y     = "Residuals"
  ) +
  theme_classic()
```

```{r}
# Evaluate the normality of residuals
residuals_tbl %>%
  ggplot(aes(sample = Residuals)) +
  geom_qq() +
  geom_qq_line() +
  labs(
    title = "Evaluate the normality of residuals",
    x     = "Theoretical Quantiles",
    y     = "Sample Quantiles"
  ) +
  theme_classic()
```

```{r}
# Evaluate the homoscedasticity of residuals
residuals_tbl %>% 
  ggplot(aes(Predicted, Residuals)) +
  geom_point() +
  geom_hline(
    yintercept = 0,
    color      = "red",
    linetype   = "dashed"
  ) +
  labs(
    title = "Evaluate the homoscedasticity of residuals",
    x     = "Predicted Values",
    y     = "Residuals"
  ) +
  theme_classic()
```

# Basic Web Scraping and APIs

## Web Scraping from Wikipedia

```{r}
# Function to scrape Wikipedia page
scrape_wikipedia <- function(url) {
  page <- url %>%
    read_html()
  
  title <- page %>%
    html_elements("title") %>%
    html_text()
  
  paragraph <- page %>%
    html_elements("p") %>%
    html_text() %>%
    extract2(3)
  
  table <- page %>%
    html_elements(xpath = "//*[@id='mw-content-text']/div[1]/table[2]") %>%
    html_table() %>%
    extract2(1)
  
  return(
    list(
      title     = title,
      paragraph = paragraph,
      table     = table
    )
  )
}

# Target Page
url <- "https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)"

# Scrape Wikipedia page
wiki_data <- scrape_wikipedia(url)

# Extract table
gdp_tbl <- wiki_data$table

# Clean the table
gdp_clean <- gdp_tbl %>%
  rename(
    country                     = 1,
    un_region                   = 2,
    gdp_estimate_imf            = 3,
    year_imf                    = 4,
    gdp_estimate_world_bank     = 5,
    year_world_bank             = 6,
    gdp_estimate_united_nations = 7,
    year_united_nations         = 8
  ) %>%
  slice(-c(1:2)) %>%
  pivot_longer(
    cols           = starts_with("gdp_estimate"),
    names_to       = "source",
    names_prefix   = "gdp_estimate_",
    values_to      = "gdp_estimate",
    values_drop_na = TRUE
  ) %>%
  pivot_longer(
    cols           = starts_with("year"),
    names_to       = "year_source",
    names_prefix   = "year_",
    values_to      = "year",
    values_drop_na = TRUE
  ) %>%
  filter(str_remove(source, "gdp_estimate_") == str_remove(year_source, "year_")) %>%
  select(-year_source) %>%
  mutate(
    gdp_estimate = gdp_estimate %>% str_replace_all(",", "") %>% as.numeric(),
    year = year %>% str_replace_all("\\[.*\\]", ""),
    across(where(~ is.character(.x)), as.factor)
  )
```

```{r}
# Visualizations
gdp_clean %>%
  summarize(
    avg_estimate = mean(gdp_estimate),
    .by = c(un_region, country)
  ) %>%
  arrange(desc(avg_estimate)) %>%
  slice_head(n = 20) %>%
  ggplot(aes(fct_reorder(country, avg_estimate), avg_estimate, fill = un_region)) +
  geom_col() +
  scale_y_continuous(
    expand = c(0, 0),
    labels = scales::label_dollar(),
    limits = c(0, 27000000)
  ) +
  scale_fill_viridis_d(
    option = "turbo",
    direction = -1 
  ) +
  coord_flip() +
  labs(
    fill     = "UN Region",
    title    = "Top 20 countries with highest nominal GDP (USD Million)",
    subtitle = "Average of estimates made by IMF, World Bank & UN (2020-2022)",
    x        = NULL,
    y        = NULL,
    caption  = "Data Source: Wikipedia | Visualization: Antti Rask"
  ) +
  theme_classic() +
  theme(
    panel.grid.major.x = element_line()
  )
```

## API

```{r}
# API URL
url = "https://api.fiscaldata.treasury.gov/services/api/fiscal_service/v1/accounting/mts/mts_table_1"

# Fetch data from API
treasury_api <- GET(url)
```

```{r}
# Check the content of the API response
str(content(treasury_api), max.level = 3)
```

```{r}
# Parse JSON data from API response
result  <- content(treasury_api,"text", encoding = "UTF-8")
df_json <- fromJSON(result, flatten = TRUE)
tbl     <- as_tibble(df_json$data)
```

```{r}
# Save tibble to CSV
write_csv(tbl, "data/Monthly_Treasury_Statement.csv")
```

