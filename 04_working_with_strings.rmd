---
title: "Data Wrangling with R, Chapter 4: Working with Strings"
author: "Gustavo R. Santos (original) | Antti Rask (modifications)"
date: "2022-07-03"
output: html_document
---

# Introduction to StringR

## Load Libraries

```{r, message=FALSE, warning=FALSE}
library(conflicted) # An Alternative Conflict Resolution Strategy
  conflicts_prefer(dplyr::filter)
library(ggraph)     # An Implementation of Grammar of Graphics for Graphs and Networks
library(gmodels)    # Various R Programming Tools for Model Fitting
library(gutenbergr) # Download and Process Public Domain Works from Project Gutenberg
library(igraph)     # Network Analysis and Visualization
library(janitor)    # Simple Tools for Examining and Cleaning Dirty Data
library(knitr)      # A General-Purpose Package for Dynamic Report Generation in R
library(SnowballC)  # Snowball Stemmers Based on the C 'libstemmer' UTF-8 Library
library(tidytext)   # Text Mining using 'dplyr', 'ggplot2', and Other Tidy Tools
library(tidyverse)  # Easily Install and Load the 'Tidyverse'
library(textstem)   # Tools for Stemming and Lemmatizing Text
library(tm)         # Text Mining Package
```

## Creating a String

```{r}
string1 <- "I am a string"
string2 <- "me too"
string3 <- 'quote inside "quote", use single quote for the string and double for the text.'

# Printing
string1
writeLines(string2)
cat(string3)
```

## Detecting Patterns

```{r}
# Create a string
text <- "hello"

# Detect if the string has the letters "rt"
str_detect(text, "rt")

# Detect if the string has the letters "ll"
str_detect(text, "ll")

# Create a phrase
text <- "hello, world!"

# Detect if the string has the word "world"
str_detect(text, "world")
```

### Determine If a String Starts with a Pattern

```{r}
# Create strings
my_id <- "ORDER-1234"

# Starts with
str_starts(my_id, "ORDER")
```

### Find the Index of a pattern

```{r}
# Create vector
shop_list <- c("fruit", "vegetable", "pasta")

# Index of "pasta"
str_which(shop_list, "pasta")
```

### Find Location of a Pattern

```{r}
shop_list <- c("fruit", "vegetable", "pasta")

str_locate(shop_list, "pasta")
```

### Counting Pattern

```{r}
# Text
text <- "I want want want to count the repetitions of want in this phrase. Do you want the same"

# Count occurrences of want
str_count(text, "want")
```

### Subset Strings

#### Extract Only the Text Between the Start and End Defined in the Function.

```{r}
# Create strings
my_id <- "ORDER-1234"

# Count occurrences of want
str_sub(
  my_id,
  start = 1,
  end   = 5
)
```

#### Return Only the Matches

```{r}
# Text
my_ids <- c("ORDER-1234", "ORDER-2234", "MAINT-1234", "MAINT-2234")

# Return orders
str_subset(my_ids, "ORDER")
```

## Manage Lengths

### Length of Strings

```{r}
# text
text <- "What is the size of this string?"

# Length
str_length(text)
```

### Trim

```{r}
# text
text <- " Text    to  be trimmed. "

#trim
str_trim(text, side = "both")

# squish
str_squish(text)
```

## Mutate Strings

### Change Letter Case

```{r}
# text
text <- "Hello world."

# to UPPERCASE
str_to_upper(text)

# to lowercase
str_to_lower(text)

# to Title Case
str_to_title(text)
```

### Replace a Pattern

```{r}
# text
text <- "Hello world. The world is beautiful!"

# Replace a pattern
str_replace(text, "world", "day")

# Replace all the patterns at once
str_replace_all(text, "world", "day")
```

## Join and Split

### Join

```{r}
# text
s1 <- "Hello"
s2 <- "world!"

# concatenate
str_c(s1, s2, sep = " ")
```

### Split

```{r}
# text
text <- "I am learning how to split strings"

# split
str_split(text, pattern = " ")
```

## Order Strings

```{r}
# text
shop_list <- c("bananas", "strawberries", "avocado", "pasta")

# ordinate
str_sort(shop_list, decreasing = FALSE)
```

# Working with Regular Expressions

## Escape Patterns

```{r}
# text
txt <- "Looking for a ."

# Regex escape
str_view_all(txt, pattern = "\\.")
```

## Finding Patterns

### Find a Single Character.

```{r}
# text
txt <- "This is a text for us to learn RegEx 101! Ok?"

# Find a single character
str_view_all(txt, "r")    # for single character
str_view_all(txt, "rn")   # for multiple characters
str_view_all(txt, "[rn]") # [] for multiple single characters
```

### Find Numbers, Text or Punctuation

```{r}
# Find numbers
str_view_all(txt, "[:digit:]")

# Find letters
str_view_all(txt, "[:alpha:]")

# Find punctuation
str_view_all(txt, "[:punct:]")

# Find spaces
str_view_all(txt, "[:space:]")
```

## Basic Codes

```{r}
# text
txt <- "This is a text for us to learn RegEx 101! Ok?"

# Find a single character (case sensitive)
"Find a single character"
str_view_all(txt, "[a-zA-Z]")
str_view_all(txt, "[a-z]")

# Not match: is
"Not match: is"
str_view_all(txt, "[^is]")
```

```{r}
# Extract digits
"Extract digits"
str_extract_all(txt, "\\d")

# Extract non-digits
"Extract non-digits"
str_extract_all(txt, "\\D")

# Extract 'words'
"Extract 'words'"
str_extract_all(txt, "\\w")

# Extract non-'words'
"Extract non-'words'"
str_extract_all(txt, "\\W")

# Extract white spaces
"Extract white spaces"
str_extract_all(txt, "\\s")

# Extract non-white spaces
"Extract non-white spaces"
str_extract_all(txt, "\\S")
```

```{r}
# Begins with T
"Begins with T"
str_view_all(txt, "^T")

# Ends with ?
"Ends with ?"
str_view_all(txt, "\\?$")

# Boundaries
"Boundaries"
str_view_all(txt, "\\btext\\b")

# Pattern can happen zero or more times
"Pattern can happen zero or more times"
str_view_all(txt, "[E*]")

# Pattern can happen one or more times
"Pattern can happen one or more times"
str_view_all(txt, "R+")
```

# Creating Frequency Data Summaries in R

```{r}
# Download Alice's Adventures in Wonderland
alice <- gutenberg_download(gutenberg_id = 11)

alice
```

## Basic Counts

```{r}
# Exact Match of Alice
"Exact Match of Alice"
alice %>%
  pull(text) %>% 
  str_count("Alice") %>%
  sum()

# How many digits in the text?
"How many digits in the text?"
alice %>%
  pull(text) %>% 
  str_count("[:digit:]") %>%
  sum()

# How many written numbers 1-5. | means “or”.
"How many written numbers 1-5. | means 'or'."
alice %>%
  pull(text) %>% 
  str_count("one|two|three|four|five") %>%
  sum()

# How many words in UPPERCASE
"How many words in UPPERCASE"
alice %>%
  pull(text) %>% 
  str_count("\\b[A-Z]+\\b") %>%
  sum()

# Count the gerund words, ending in "ing"
"Count the gerund words, ending in 'ing'"
alice %>%
  pull(text) %>% 
  str_count("\\b(\\w+ing)") %>%
  sum()
```

## Gerund Words

```{r}
# Extract the gerund words, ending in "ing"
gerunds <- alice %>%
  pull(text) %>%
  str_extract_all("\\b(\\w+ing)")

# Show only the values
unlist(gerunds)
```

```{r}
# Show only the unique values (regardless of the original case)
ings <- gerunds %>% 
  unlist() %>%
  str_remove_all("_") %>%
  str_to_lower() %>%
  str_sort()

ings %>% 
  str_unique()
```

## Data summary of the Words Ending with "ing" in the Book

```{r}
# Let's count the unique words
ings_tbl <- ings %>%
  as_tibble() %>%
  count(value, name = "n") %>% 
  arrange(desc(n))

ings_tbl
```

## Complete Data Summary of the Main Characters of the Book

```{r}
# List characters
characters <- c("Alice", "Rabbit", "Queen", "King", "Cheshire Cat", "Duchess", "Caterpillar", "Hatter")

# Create regex string
char_regex <- characters %>% 
  str_c(collapse = "|")

char_regex
```

```{r}
# Make the entire text Title Case for better match
alice_title_case <- alice %>% 
  pull(text) %>%
  str_to_title()

# Extract the words
book_chars <- alice_title_case %>%
  str_extract_all(char_regex) %>% 
  unlist()

# Data Frequency to data.frame
book_chars_tbl <- book_chars %>%
  as_tibble() %>%
  count(value, name = "n") %>%
  arrange(desc(n))

book_chars_tbl
```

### Enhance the Frequency Table

```{r}
book_chars_tbl %>% 
  arrange(desc(n)) %>% 
  mutate(
    pct        = prop.table(n) %>% round(3),
    pct_cumsum = cumsum(pct)
  )
```

## Contingency table

```{r}
#Load the data
data("mtcars")

# Create the contingency table
mtcars %>% 
  tabyl(cyl, gear) %>%
  adorn_totals(c("row", "col")) %>%
  adorn_percentages("row") %>% 
  adorn_pct_formatting(rounding = "half up", digits = 1) %>%
  adorn_ns() %>%
  adorn_title("combined")
```

## Factors

```{r}
# Textual variable
var <- c("A", "B", "B", "C", "A", "C")

# To create a factor
factor_var <- factor(var)

levels(factor_var)
```

### Levels with Hierarchical Order

```{r}
# Ordered levels
levels(factor_var) <- c("C","B","A")

factor_var
```

# Text Mining with {tidytext}

```{r}
# Downloading "The Time Machine" by H. G Wells
book <- gutenberg_download(gutenberg_id = 35)

book
```

## Tokenization

```{r}
tokenized_book <- book %>% 
  unnest_tokens(input = text, output = "tokens") %>% 
  select(-gutenberg_id)

tokenized_book
```

## Cleaning Stop Words

```{r}
clean_tokens <- tokenized_book %>%
  anti_join(stop_words, by = c("tokens" = "word"))

clean_tokens
```

## Word Count

```{r}
word_count <- clean_tokens %>% 
  count(tokens, sort = TRUE)

word_count
```

## Stemming

```{r}
stemmed_tokens <- clean_tokens %>%
  mutate(stem = wordStem(tokens)) %>%
  count(stem, sort = TRUE)

stemmed_tokens
```

## Lemmatization

```{r}
clean_corpus <- function(text) {
  # Transform text to Corpus, so we have the functions
  corp <- Corpus(VectorSource(text)) %>%
    # All text to lowercase
    tm_map(tolower) %>%
    # Text to Plain text
    tm_map(PlainTextDocument) %>%
    # Removing numbers
    tm_map(removeNumbers) %>%
    # Removing punctuation
    tm_map(removePunctuation) %>%
    # Removing stopwords
    tm_map(removeWords, c(stopwords("english"), "...", '”', "—")) %>%
    # Strip white space
    tm_map(stripWhitespace)
}
```

```{r, warning=FALSE}
# Clean text
clean_text <- book %>% 
  pull(text) %>% 
  clean_corpus()

# Lemmatization
clean_lemmatized <- tm_map(clean_text, lemmatize_strings)
TDM              <- TermDocumentMatrix(clean_lemmatized)
TDM              <- as.matrix(TDM)
word_frequency   <- sort(rowSums(TDM), decreasing = TRUE)
lemm_tbl         <- tibble(
  word = names(word_frequency),
  freq = word_frequency
) %>% 
  filter(!word %in% c('“', '”'))

lemm_tbl
```

## Term Frequency - Inverse Document Frequency [TF-IDF]

```{r}
tokens_by_chapter <- clean_tokens %>% 
  mutate(idx = row_number()) %>% 
  select(idx, tokens)

idx <- tokens_by_chapter %>% 
  filter(
    tokens %in%
      c(
        "ii",
        "iii",
        "iv",
        "v",
        "vi",
        "vii",
        "viii",
        "ix",
        "x",
        "xi",
        "xii",
        "xiii",
        "xiv",
        "xv",
        "xvi", 
        "introduction",
        "epilogue"
      ) &
      idx > 46
  )

tokens_by_chapter <- tokens_by_chapter %>% 
  mutate(chapter = 
           case_when(
             idx < 621                  ~ "introduction",
             between(idx, 621, 1045)    ~ "ii",
             between(idx, 1045, 1764)   ~ "iii",
             between(idx, 1764, 2566)    ~ "iv",
             between(idx, 2566, 3143)   ~ "v",
             between(idx, 3143, 3930)   ~ "vi",
             between(idx, 3930, 4695)   ~ "vii",
             between(idx, 4695, 6098)   ~ "viii",
             between(idx, 6098, 6825)   ~ "ix",
             between(idx, 6825, 7670)   ~ "x",
             between(idx, 7670, 8509)   ~ "xi",
             between(idx, 8509, 9407)   ~ "xii",
             between(idx, 9407, 9777)   ~ "xiii",
             between(idx, 9777, 10537)  ~ "xiv",
             between(idx, 10537, 10700) ~ "xv",
             between(idx, 10700, 11171) ~ "xvi",
             idx >= 11171               ~ "epilogue"
           )
  ) %>% 
  select(chapter, tokens)

# Words by chapter
words_by_chapter <- tokens_by_chapter %>%
  count(tokens, chapter, sort = TRUE)

# TF-IDF calculation
data_tf_idf <- words_by_chapter %>%
  bind_tf_idf(tokens, chapter, n)

data_tf_idf
```

## Plot TF-IDF

```{r}
plot_tf_idf <- data_tf_idf %>%
  group_by(chapter) %>%
  slice_max(tf_idf, n = 5, with_ties = FALSE) %>%
  ungroup()

plot_tf_idf %>% 
  ggplot(aes(tf_idf, fct_reorder(tokens, tf_idf), fill = chapter)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(vars(chapter), ncol = 4, scales = "free") +
  labs(
    x     = "tf-idf",
    y     = NULL,
    title = "Term Frequency - Inverse Document Frequency (TF-IDF) by Chapter"
  ) +
  theme_classic()
```

## N-grams

### Most frequent 2-grams

```{r}
book %>%
  unnest_tokens(
    input  = text,
    output = ngrams,
    token  = "ngrams",
    n      = 2
  ) %>% 
  count(ngrams, sort = TRUE)
```

### Most frequent 2-grams using clean data

```{r, warning=FALSE}
clean_text <- clean_tokens %>% 
  select(tokens) %>% 
  str_c(
    sep      = " ",
    collapse = NULL
  ) %>% 
  as_tibble()

clean_text %>%
  unnest_tokens(
    input  = value,
    output = ngrams,
    token  = "ngrams",
    n      = 2
  ) %>% 
  count(ngrams, sort = TRUE)
```

## Plot Network of Related Words and Chapters

```{r}
word_freq <- clean_tokens %>%
  count(tokens, sort = TRUE)

set.seed(1234)
words_by_chapter %>%
  filter(n >= 8) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(
    aes(
      edge_alpha = n,
      edge_width = n
    ),
    edge_color = "cyan4"
  ) +
  geom_node_point(size = 5) +
  geom_node_text(
    aes(label = name),
    repel         = TRUE,
    point.padding = unit(0.2, "lines")
  ) +
  theme_void()
```
