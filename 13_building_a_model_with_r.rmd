---
title: "13 Building a Model with R"
author: "Gustavo R. Santos (original) | Antti Rask (modifications)"
date: "2022-08-31"
output: html_document
---

# Building a Model with R

## Dataset

The dataset to be used in this project is the _Spambase_ from the UCI Machine Learning Repository.

__Dataset Credits:__
Creators:
Mark Hopkins, Erik Reeber, George Forman, Jaap Suermondt

Donor:
George Forman

__URL Address__
https://archive.ics.uci.edu/ml/datasets/spambase

## Loading libraries

```{r, message=FALSE, warning=FALSE}
library(caret)           # Classification and Regression Training
library(conflicted)      # An Alternative Conflict Resolution Strategy
  conflicts_prefer(dplyr::filter)
  conflicts_prefer(pdp::partial)
library(gghighlight)     # Highlight Lines and Points in 'ggplot2'
library(ggRandomForests) # not installed on this machine 
library(patchwork)       # The Composer of Plots
library(pdp)             # Partial Dependence Plots
library(randomForest)    # Breiman and Cutler's Random Forests for Classification and Regression
library(ROCR)            # Visualizing the Performance of Scoring Classifiers
library(skimr)           # Compact and Flexible Summaries of Data
library(tidyverse)       # Easily Install and Load the 'Tidyverse'
```

### Loading Dataset

```{r}
# Link where the dataset is located in UCI database 
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data"

# String of headers
headers <- c(
  "word_freq_make",
  "word_freq_address",
  "word_freq_all",
  "word_freq_3d",
  "word_freq_our",
  "word_freq_over",
  "word_freq_remove",
  "word_freq_internet",
  "word_freq_order",
  "word_freq_mail",
  "word_freq_receive",
  "word_freq_will",
  "word_freq_people",
  "word_freq_report",
  "word_freq_addresses",
  "word_freq_free",
  "word_freq_business",
  "word_freq_email",
  "word_freq_you",
  "word_freq_credit",
  "word_freq_your",
  "word_freq_font",
  "word_freq_000",
  "word_freq_money",
  "word_freq_hp",
  "word_freq_hpl",
  "word_freq_george",
  "word_freq_650",
  "word_freq_lab",
  "word_freq_labs",
  "word_freq_telnet",
  "word_freq_857",
  "word_freq_data",
  "word_freq_415",
  "word_freq_85",
  "word_freq_technology",
  "word_freq_1999",
  "word_freq_parts",
  "word_freq_pm",
  "word_freq_direct",
  "word_freq_cs",
  "word_freq_meeting",
  "word_freq_original",
  "word_freq_project",
  "word_freq_re",
  "word_freq_edu",
  "word_freq_table",
  "word_freq_conference",
  "char_freq_semicolon",
  "char_freq_parenthesis",
  "char_freq_squarebrkt",
  "char_freq_exclam",
  "char_freq_dollar",
  "char_freq_hashtag",
  "capital_run_length_average",
  "capital_run_length_longest",
  "capital_run_length_total",
  "spam"
)

# Load the dataset
spam <- read_csv(
  url,
  col_names = headers,
  trim_ws   = TRUE
)

# Clean R environment
remove(headers, url)

spam
```

### Understanding the Data

* This dataset has 4601 rows and 58 columns (57 explanatory variables and 1 target).
* Once we look at the glimpse, we should notice that all the variables are of the type _double_, or real numbers.
* As per the documentation of the dataset, the numbers for the explanatory variables starting with _word_freq_WORD_ mean the percentage of words in the e-mail that match "WORD". 
* The same is valid for char_freq_CHAR.
* The variables _capital_run_length__ are measurements related to the quantity of consecutive letters written in caps.

```{r}
# Data dimensions
dim(spam)
```

```{r}
# Glimpse of the data
glimpse(spam)
```

Once the dataset is correctly loaded, the next step is to convert the variable types as needed.

The target variable _spam_ will be converted to factor.

The three _capital_run_length_xxx_ will be converted to integer.

```{r}
# Columns to change to factor
cols_to_int    <- c("capital_run_length_average", "capital_run_length_longest", "capital_run_length_total")
cols_to_factor <- c("spam")

# Assign variables as factor
spam_cleaned <- spam %>% 
  mutate(across(all_of(cols_to_int), as.integer)) %>% 
  mutate(across(all_of(cols_to_factor), factor))

# Check result
glimpse(spam_cleaned)

# Remove variable to keep environment clean
remove(cols_to_factor, cols_to_int, spam)
```

### Missing Data

Let's look if this dataset has missing values.

```{r}
# Check for NA
spam_cleaned %>%
  is.na() %>%
  sum()
```

There are no missing values, so we can move on with the exploration.

### Exploratory Analysis

In the exploration phase, we can start with the descriptive statistics, to learn more about the variables distributions.

```{r}
# Configure display not scientific numbers
options(scipen = 999, digits=3)

# Descriptive statistics
skim(spam_cleaned)
```

* We can note that there are many means close to zero, therefore the majority of the variables never go too high. 
* There are some variables with more than 75% of the observations with zeroes.
* The stantard deviations are high, denoting presence of outliers or highly spread data.
* The histograms may be skewed to the right.

Let's check the histograms next.

#### Histograms

After looking at the descriptive statistics, we can plot the histograms to have a visualization of the distributions.

```{r}
# Define a function to create a histogram for a single variable
create_histogram <- function(data, var_name) {
  data %>%
    ggplot(aes(.data[[var_name]])) +
    geom_histogram(
      fill  = "royalblue",
      color = "black",
      bins  = 10
    ) +
    scale_y_continuous(expand = c(0, 0)) +
    labs(
      title = str_c("Histogram of ", var_name),
      x     = var_name,
      y     = "Count"
    ) +
    theme_classic()
}

# Apply the function to each variable and save the plots in a list
histograms <- map(colnames(spam_cleaned %>% select(1:57)), ~ create_histogram(spam_cleaned, .x))

# Print the histograms one by one (optional)
walk(histograms, print)
```

All the histograms show values concentrated around the zero.

#### Boxplots

Let's check for the presence of outliers

```{r}
# Define a function to create a boxplot for a single variable
create_boxplot <- function(data, var_name) {
  data %>%
    ggplot(aes(.data[[var_name]])) +
    geom_boxplot(fill = "royalblue", color = "black") +
    coord_flip() +
    labs(
      title = str_c("Boxplot of ", var_name),
      x     = var_name,
      y     = NULL
    ) +
    theme_classic()
}

# Apply the function to each variable and save the plots in a list
boxplots <- map(colnames(spam_cleaned %>% select(1:57)), ~ create_boxplot(spam_cleaned, .x))

# Print the boxplots one by one (optional)
walk(boxplots, print)
```

As expected after we checked the descriptive statistics, we confirmed that the data has a lot of outliers.

Next, we will check what is the proportions of Spam and Not Spam in the dataset. Our expectation is to see usually numbers close to the 50-50 ratio.

```{r}
# Spam vs Not Spam proportion to a tibble
spam_prop <- spam_cleaned %>%
  count(spam) %>%
  mutate(Freq = n / sum(n)) %>%
  select(-n)

# Spam vs Not Spam proportion
spam_prop %>%
  ggplot(aes(spam, Freq)) + 
  geom_col(fill = "royalblue") +
  geom_text(
    aes(label = str_c(round(Freq, 1) * 100, "%")),
    vjust  = 2,
    color = "white"
  ) +
  scale_x_discrete(labels = c("1" = "Spam", "0" = "Not spam")) +
  labs(
    title = "Proportion of each label in the dataset",
    x     = NULL,
    y     = NULL
    ) +
  theme_classic() +
  theme(
    axis.line   = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks  = element_blank()
  )
```

```{r}
# Pivot the table to Long format 
long_spam <- spam_cleaned %>% 
  pivot_longer(
    cols      = 1:57,
    names_to  = "words",
    values_to = "pct"
  )

# Top 10 plot
long_spam %>%
  filter(str_detect(words,"word_") & spam == 1) %>%
  ggplot(aes(pct, fct_reorder(words, pct), fill = spam)) +
  geom_boxplot() +
  labs(
    title    = "Percentages of words and their association with spam e-mails",
    subtitle = "The frequency of appearance of some words in e-mails is more associated with spam.",
    x        = NULL,
    y        = NULL
  ) + 
  theme_classic() + 
  theme(
    legend.position = "none",
    plot.subtitle   = element_text(color = "darkgray", size = 11)
    )
```

Top 20 plot.

```{r}
# Median
top20 <- long_spam %>% 
  filter(str_detect(words,"word_") & spam == 1) %>%
  summarise(
    mean_pct = mean(pct),
    .by      = words
  ) %>% 
  arrange(desc(mean_pct)) %>%
  slice_head(n = 20)

# Top 20 plot
long_spam %>% 
  filter(str_detect(words,"word_") & spam == 1 & words %in% top20$words) %>%
  ggplot(aes(pct, fct_reorder(words, pct), fill = spam))+
  geom_boxplot() +
  labs(
    title    = "Percentages of words and their association with spam",
    subtitle = "The frequency of appearance of some words in e-mails is associated with spam.",
    x        = NULL,
    y        = NULL
  ) + 
  theme_classic() + 
  theme(
    legend.position = "none",
    plot.subtitle   = element_text(color = "darkgray", size = 11)
  )

# Clean environment
remove(top20, long_spam)
```

#### Spam words

Let"s check the impact of the top 23 words more associated with spam e-mails and how it affects the message being considered spam or not.

```{r}
# Define top words
top_words <- c(
  "word_freq_you",
  "word_freq_your",
  "word_freq_will",
  "word_freq_free",
  "word_freq_our",
  "word_freq_all",
  "word_freq_mail",
  "word_freq_email",
  "word_freq_business",
  "word_freq_remove",
  "word_freq_000",
  "word_freq_font",
  "word_freq_money",
  "word_freq_internet",
  "word_freq_credit",
  "word_freq_over",
  "word_freq_order",
  "word_freq_3d",
  "word_freq_address",
  "word_freq_make",
  "word_freq_people",
  "word_freq_re",
  "word_freq_receive",
  "spam"
)

# Select only columns with top words
top_tbl <- spam_cleaned %>%
  select(all_of(top_words)) %>% 
  # Add total percentage
  mutate(top_w_pct = rowSums(across(where(is.numeric))))

# Plot bar graphic for top 23 words
g1 <- top_tbl %>% 
  ggplot(aes(top_w_pct, spam)) +
  geom_boxplot(fill = "royalblue") +
  labs(title = "Top 23 words")

# Select the entire dataset
spam2 <- spam_cleaned %>% 
  # Add total percentage
  mutate(w_pct = rowSums(across(where(is.numeric))))

# Plot bar graphic for the entire dataset
g2 <- spam2 %>% 
  ggplot(aes(w_pct, spam)) +
  geom_boxplot(fill = "royalblue") +
  labs(title = "Entire dataset")

# patchwork
((g1|g2) +
  plot_annotation(
    title    = "How spam associated words impacts the classification",
    subtitle = "Spam emails have a higher percentage of those words."
)) *
  scale_y_discrete(labels = c("1" = "Spam", "0" = "Not Spam")) *
  labs(y = NULL) *
  theme_classic()
```
The boxplot shows us a similar result of T-tests to compare averages. Therefore, if we perform the statistical test of Kolmogorov-Smirnov (*it tests the differences between the empirical cumulative distributions of two non-normal samples*), it should confirm that the samples are statistically different.

__Hypothesis test:__
* Ho: (p > 0.05) samples not statistically different
* Ha: (p <= 0.05) samples are statistically different

```{r}
# Kolmogorov-Smirnov test
yes_spam <- top_tbl %>%
  filter(spam == 1) %>%
  pull(top_w_pct)

not_spam <- top_tbl %>%
  filter(spam == 0) %>%
  pull(top_w_pct)

ks.test(yes_spam, not_spam)
```

Since the _p-value_ returned very close to zero, we have enough statistical evidence to reject the null hypothesis in favor of the alternative, confirming that the samples are not equal.

Testing the impact of characters in the classification of the email as spam or not.

```{r}
# Add 

# Define business words
char_words <- c(
  "char_freq_semicolon",
  "char_freq_parenthesis",
  "char_freq_squarebrkt",
  "char_freq_exclam",
  "char_freq_dollar",
  "char_freq_hashtag",
  "spam"
)

# Select only columns with characters
char_tbl <- spam_cleaned %>% 
  select(all_of(char_words)) %>% 
  # Add total percentage
  pivot_longer(
    cols      = -spam,
    names_to  = "character",
    values_to = "char_pct"
  ) %>% 
  filter(spam == 1)

# Plot bar graphic
char_tbl %>% 
  ggplot(aes(char_pct, fct_reorder(character, char_pct), fill = spam)) +
  geom_boxplot() +
  labs(
    title    = "How the presence of charachters in e-mails impacts the classification",
    subtitle = "The spam emails have a higher percentage of those characters.",
    x        = NULL,
    y        = NULL
  ) +
  theme_classic() +
  theme(
    legend.position = "none"
  )
```

Kolmogorov-Smirnov Test of samples

```{r}
#Characters only
char_tbl <- spam_cleaned %>% 
  select(all_of(char_words)) %>% 
  # Add total percentage
  pivot_longer(
    cols      = -spam,
    names_to  = "character",
    values_to = "char_pct"
  )

# Kolmogorov-Smirnov test
yes_spam <- char_tbl %>%
  filter(spam == 1) %>%
  pull(char_pct)

not_spam <- char_tbl %>%
  filter(spam == 0) %>%
  pull(char_pct)

ks.test(yes_spam, not_spam)
```

There is an impact whether there are many characters or not.

Testing the impact of CAPITAL letters in the classification of the email as spam or not.

```{r}
# Define variables with capital letters
capital <- c(
  "capital_run_length_average",
  "capital_run_length_longest",
  "capital_run_length_total",
  "spam"
)

# Select only columns with business words
caps_tbl <- spam_cleaned %>%
  select(all_of(capital))

# Plot bar graphic
g1 <- caps_tbl %>%
  ggplot(aes(capital_run_length_average, spam)) +
  labs(title = "Capital Letters Avg")

g2 <- caps_tbl %>% 
  ggplot(aes(capital_run_length_longest, spam)) +
  labs(title = "Capital Letters Longest")

g3 <- caps_tbl %>% 
  ggplot(aes(capital_run_length_total, spam)) +
  labs(title = "Capital Letters Total")

# Patchwork - All in the same figure side-by-side
((g1|g2|g3) +
  plot_annotation(title = "Capital Letters vs. Spam Classification")
) *
  geom_boxplot(fill = "royalblue") *
  scale_y_discrete(labels = c("1" = "Spam", "0" = "Not Spam")) *
  labs(
    x = NULL,
    y = NULL
  ) *
  theme_classic()

remove(g1, g2, g3)
```

Kolmogorov-Smirnov Test of samples

```{r}
# Kolmogorov-Smirnov test
yes_spam <- caps_tbl %>%
  filter(spam == 1) %>%
  pull(capital_run_length_total)

not_spam <- caps_tbl %>%
  filter(spam == 0) %>%
  pull(capital_run_length_total)

ks.test(yes_spam, not_spam)

# Clean environment
remove(yes_spam, not_spam)
```

After the exploration and testing, we conclude that there are a few words more associated with spam, the presence of characters and capital letters.

Let's transform our data to make it simpler and more generalizable before modeling.

### Transforming the data

The transformation will be to combine the top 20 words in a single variable, characters in another and the total of capitals in a third variable.

```{r}
# Creating the dataset input for modeling

# Add column top_w_pct
spam_for_model <- spam_cleaned %>%
  bind_cols(top_w_pct = top_tbl %>% pull(top_w_pct)) %>% 
  # Select needed variables for modeling
  select(
    spam,
    top_w_pct,
    char_freq_exclam,
    char_freq_parenthesis,
    char_freq_dollar,
    capital_run_length_total,
    capital_run_length_longest
  ) %>%
  # Replace the binary 1(spam) and 0(not_spam)
  mutate(spam = recode(spam, "1" = "is_spam", "0" = "not_spam"))

# Clean environment
remove(char_tbl, caps_tbl, char_words, top_tbl, top_words, capital)

spam_for_model
```

### Modeling

```{r}
# Seed to reproduce the same result for random numbers
set.seed(17)

# Create index for random train test split
number_of_rows <- nrow(spam_for_model)
idx <- sample(1:number_of_rows, size = 0.8 * number_of_rows)

# Split in train and test datasets
train <- spam_for_model %>%
  slice(idx) %>%
  drop_na()

test <- spam_for_model %>%
  slice(-idx) %>%
  drop_na()
```

```{r}
# Label proportions of the train set
cat("Train set:")
prop.table(table(train %>% pull(spam)))
writeLines("=================")

# Label proportions of the test set
cat("Test set:")
prop.table(table(test %>% pull(spam)))
```

```{r}
# Training the model
rf <- randomForest(
  spam ~ .,
  data       = train,
  importance = TRUE,
  ntree      = 250
)

rf
```

Let's see the performance of the trained model.

```{r}
# Extract error rates from the randomForest object and create a tidy data frame
error_rates <- rf$err.rate %>%
  as_tibble(rownames = "Trees") %>%
  pivot_longer(
    cols      = -Trees,
    names_to  = "Error_Type",
    values_to = "Error_Rate"
  ) %>%
  mutate(Trees = as.integer(Trees))

error_rates
```

```{r}
# Plot the error rates using ggplot2
error_rates %>% 
  ggplot(aes(Trees, Error_Rate, color = Error_Type)) +
  geom_line() +
  scale_color_manual(
    values = c("red", "blue", "green"),
    labels = c("OOB Error Rate", "Type 1 Error Rate (False Positive)", "Type 2 Error Rate (False Negative)")
  ) +
  scale_y_continuous(
    breaks = seq(0.04, 0.20, by = 0.02),
    limits = c(0.04, 0.20)
  ) +
  gghighlight(
    label_key       = Error_Type,
    line_label_type = "sec_axis"
  ) +
  scale_x_continuous(expand = c(0.01, 0)) +
  labs(
    title = "Random Forest Model Performance",
    x     = "Number of Trees",
    y     = "Error Rate",
    color = "Error Type"
  ) +
  theme_classic()
```

```{r}
# Predictions
preds <- predict(rf, test)
```

### Evaluating Errors

Evaluating the model is very important to to know where the model is making mistakes.

The first step is to look at the confusion matrix, as that will show us the false positives (Type 1 error: say it is spam when it is not) and the false negatives (Type 2 error: say it is not_spam when it is)

```{r}
# Confusion Matrix
confusionMatrix(preds, test$spam)
```

Checking variables importance

```{r}
# Obtain variable importance using the importance() function
var_importance <- importance(rf) %>%
  as_tibble(rownames = "Variable") %>%
  select(Variable, starts_with("MeanDecrease")) %>%
  pivot_longer(
    cols      = -Variable,
    names_to  = "Importance_Type",
    values_to = "Importance"
  )

# Plot Mean Decreased Accuracy
accuracy_plot <- var_importance %>% 
  filter(Importance_Type == "MeanDecreaseAccuracy") %>%
  ggplot(aes(reorder(Variable, Importance), Importance)) +
  labs(
    title = "Mean Decreased Accuracy",
    y     = "Importance"
  ) +
  theme_classic()

# Plot Mean Decreased Gini
gini_plot <- var_importance %>% filter(Importance_Type == "MeanDecreaseGini") %>%
  ggplot(aes(reorder(Variable, Importance), Importance)) +
  labs(
    title = "Mean Decreased Gini",
    y     = "Importance"
  ) +
  theme_classic() +
  theme(axis.text.y = element_blank())

# Combine the plots side by side using patchwork
(accuracy_plot + gini_plot) *
  geom_col(fill = "steelblue") *
  coord_flip() *
  labs(
    x = NULL
  )
```

Let's look at the partial dependency plot.

```{r}
# Define the variables of interest
vars <- c(
  "top_w_pct",
  "char_freq_exclam",
  "char_freq_parenthesis",
  "char_freq_dollar",
  "capital_run_length_total",
  "capital_run_length_longest"
)

# Function to generate the partial dependence plots
partial_plot <- function(var_name, rf_model, train_data) {
  
  pd <- partial(
    rf_model,
    pred.var = var_name,
    train    = train_data,
    plot     = FALSE
  )
  
  p <- pd %>% 
    ggplot(aes(.data[[var_name]], -yhat)) +
    geom_line() +
    labs(
      x = var_name,
      y = NULL
    ) +
    theme_classic()
  
  return(p)
}

# Generate the partial dependence plots using map
plots_list <- map(
  vars,
  partial_plot,
  rf_model   = rf,
  train_data = train
)

# Combine the plots into a 2x3 grid
(plots_list[[1]] | plots_list[[2]] | plots_list[[3]]) /
(plots_list[[4]] | plots_list[[5]] | plots_list[[6]]) +
  plot_annotation(title = "Partial Dependence Plots")
```

```{r}
# Predictions probability of not_spam
predictions <- predict(rf, test, type = "prob") %>% 
  as.data.frame() %>%
  select(not_spam)

# Predictions for ROC
pred_roc <- prediction(
  predictions %>%
    pull(not_spam),
  test %>%
    pull(spam)
)

# ROC curve
roc <- performance(pred_roc, "tpr", "fpr")

# Plot ROC curve using ggplot2
roc_data <- tibble(
  FPR = unlist(roc@x.values),
  TPR = unlist(roc@y.values)
)

roc_data %>%
  ggplot(aes(FPR, TPR)) +
  geom_path(color = "royalblue", lwd = 2) +
  geom_abline(
    intercept = 0,
    slope     = 1,
    linetype  = "dashed",
    color     = "red",
    lwd       = 1
  ) +
  labs(
    x = "False Positive Rate",
    y = "True Positive Rate"
  ) +
  theme_classic()
```

### Testing

Let's tests the model with any random text.

```{r}
text1 <- "SALE!! SALE!! SALE!! SUPER SALEEEE!! This is one of the best sales of the year! More than #3000# products with discounts up to $500 off!! Visit our page and Save $$$ now! Order your product NOW (here) and get one for free !"

text2 <- "DEAR MR. JOHN, You will find enclosed the file we talked about during your meeting earlier today. The attachment received here is also available in our web site at this address: www.DUMMYSITE.com. Sale."
```

But we know the model takes input as a data frame with 6 variables. Therefore, we will have to transform this text in the same format as the input.

Let's create a function that does the transformation.

```{r}
spam_words <- c(
  "you",
  "your",
  "will",
  "free",
  "our",
  "all",
  "mail",
  "email",
  "business",
  "remove",
  "000",
  "font",
  "money",
  "internet",
  "credit",
  "over",
  "order",
  "3d",
  "address",
  "make",
  "people",
  "re",
  "receive",
  "sale"
)
```

```{r}
prepare_input <- function(text, spam_words) {
  
  exclamation         <- str_count(text, pattern = "[!]")
  parenthesis         <- str_count(text, pattern = "[()]")
  dollar_sign         <- str_count(text, pattern = "[$]")
  total_uppercase     <- str_count(text, pattern = "[A-Z]")
  text_no_punctuation <- str_remove_all(text, pattern = "[:punct:]|[$]*")
  all_words           <- str_split(text_no_punctuation, pattern = " ")[[1]]
  
  char_counts <- map_dbl(
    all_words, function(word) {
      if (word == toupper(word)) {
        return(nchar(word))
      } else {
        return(0)
      }
    }
  )
  
  longest_upper <- max(char_counts)
  top_w         <- sum(
    map_lgl(
      all_words, function(word) {
        return(tolower(word) %in% spam_words)
      }
    )
  )
  
 input <- data.frame(
    top_w_pct                  = 100 * top_w       / length(all_words),
    char_freq_exclam           = 100 * exclamation / length(all_words),
    char_freq_parenthesis      = 100 * parenthesis / length(all_words),
    char_freq_dollar           = 100 * dollar_sign / length(all_words),
    capital_run_length_total   = total_uppercase,
    capital_run_length_longest = longest_upper
  ) %>% 
   replace_na(
     list(
       top_w_pct                  = 0,
       char_freq_exclam           = 0,
       char_freq_parenthesis      = 0,
       char_freq_dollar           = 0,
       capital_run_length_total   = 0,
       capital_run_length_longest = 0
     )
   ) %>%
   summarise_all(mean)
  
  return(input)
}
```

```{r}
# Predicting text 1
input <- prepare_input(text1, spam_words= spam_words)

# Predict
data.frame(predict(rf, input, type = "prob")) %>% 
  as_tibble()

# Predicting text 2
input <- prepare_input(text2, spam_words = spam_words)

# Predict
data.frame(predict(rf, input, type = "prob")) %>% 
  as_tibble()
```

### Saving the model

```{r}
# Saving the model
saveRDS(rf, "models/rf_model.rds")
```
